{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matan034/Knee-Bone-Segmentation/blob/main/KneeProject_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylZAVlS5lzLZ"
      },
      "source": [
        "#Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9XGtR_olxU2"
      },
      "outputs": [],
      "source": [
        "import os,glob\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQU3Y3hFl9HB",
        "outputId": "9326bc69-7c11-4eb9-94da-a5d18df0af2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "SEED = 909\n",
        "BATCH_SIZE_TRAIN = 64\n",
        "BATCH_SIZE_TEST = 64\n",
        "BATCH_SIZE_VALIDATION = 1\n",
        "\n",
        "IMAGE_HEIGHT =  256\n",
        "IMAGE_WIDTH = 256\n",
        "IMG_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "IS_BOUNDARY_PATH = False  # set this param to true for using knee boundary images\n",
        "\n",
        "if IS_BOUNDARY_PATH:\n",
        "  data_dir_train_image = '/content/drive/My Drive/KneeProject/trainingBoundary/img/'  #when using boundary images go to path trainingBoundary regular images in training\n",
        "  data_dir_train_mask = '/content/drive/My Drive/KneeProject/trainingBoundary/mask/'\n",
        "\n",
        "  data_dir_test_image = '/content/drive/My Drive/KneeProject/testingBoundary/img/'  #when creating boundary images go to path testingBoundary regular images in testing\n",
        "  data_dir_test_mask = '/content/drive/My Drive/KneeProject/testingBoundary/mask/'\n",
        "\n",
        "  data_dir_val_image = '/content/drive/My Drive/KneeProject/validationBoundary/img/' #when creating boundary images go to path validationBoundary regular images in validation\n",
        "  data_dir_val_mask = '/content/drive/My Drive/KneeProject/validationBoundary/mask/'\n",
        "else:\n",
        "   data_dir_train_image = '/content/drive/My Drive/KneeProject/training/img/'  #when using boundary images go to path trainingBoundary regular images in training\n",
        "   data_dir_train_mask = '/content/drive/My Drive/KneeProject/training/mask/'\n",
        "\n",
        "   data_dir_test_image = '/content/drive/My Drive/KneeProject/testing/img/'  #when creating boundary images go to path testingBoundary regular images in testing\n",
        "   data_dir_test_mask = '/content/drive/My Drive/KneeProject/testing/mask/'\n",
        "\n",
        "   data_dir_val_image = '/content/drive/My Drive/KneeProject/validation/img/' #when creating boundary images go to path validationBoundary regular images in validation\n",
        "   data_dir_val_mask = '/content/drive/My Drive/KneeProject/validation/mask/'\n",
        "\n",
        "\n",
        "NUM_TRAIN = len(os.listdir(data_dir_train_image+\"femur\"))\n",
        "NUM_TEST = len(os.listdir(data_dir_test_image+\"femur\"))\n",
        "NUM_VAL = len(os.listdir(data_dir_val_image+\"femur\"))\n",
        "\n",
        "NUM_OF_EPOCHS = 100\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "EPOCH_STEP_TRAIN = NUM_TRAIN // BATCH_SIZE_TRAIN\n",
        "EPOCH_STEP_TEST = NUM_TEST // BATCH_SIZE_TEST\n",
        "\n",
        "LOSS_FUNC_DICE=True       # use dice loss function\n",
        "LOSS_FUNC_TRAV=False      # use focal tversky loss\n",
        "USE_IOU = True            # use jaccard index accuracy metrics, false will use dice score\n",
        "TRANSFER_LEARNING = True  # use attention unet with transfer learning\n",
        "CLASSIFIER =False         # use attention unet with a classifer\n",
        "EXTRA_LAYER = True        # use attention unet with extra layer\n",
        "UNET_MODEL_TO_TRAIN=\"attention\" # OPTIONS : \"unet\", \"attention\", \"3plus\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o9uJFu6EwlV"
      },
      "source": [
        "# Data prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axahajake651",
        "outputId": "abca1b5f-9b75-47a1-cbff-b0c37cce08e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9314 trainig images\n",
            "9314 trainig mask\n",
            "2336 test images\n",
            "2336 test mask\n",
            "1052 validation images\n"
          ]
        }
      ],
      "source": [
        "# use for verifying amount of images in folders\n",
        "print(len(os.listdir(data_dir_train_image+\"/femur\")),\"trainig images\")\n",
        "print(len(os.listdir(data_dir_train_mask+\"/femur\")),\"trainig mask\")\n",
        "print(len(os.listdir(data_dir_test_image+\"/femur\")),\"test images\")\n",
        "print(len(os.listdir(data_dir_test_mask+\"/femur\")),\"test mask\")\n",
        "print(len(os.listdir(data_dir_val_image+\"/femur\")),\"validation images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpXJ_t-2l_zw"
      },
      "outputs": [],
      "source": [
        "# can be changed for using diffrent augmentation techniques\n",
        "def create_segmentation_generator_train(img_path, msk_path, BATCH_SIZE):\n",
        "    data_gen_args = dict(rescale=1./255,\n",
        "#                         featurewise_center=True,\n",
        "#                      featurewise_std_normalization=True,\n",
        "#                          rotation_range=90\n",
        "#                      width_shift_range=0.2,\n",
        "#                      height_shift_range=0.2,\n",
        "#                      zoom_range=0.3\n",
        "                        )\n",
        "    datagen = ImageDataGenerator(**data_gen_args)\n",
        "    class_mode_selection = 'grayscale'\n",
        "    if TRANSFER_LEARNING or CLASSIFIER:\n",
        "      class_mode_selection = 'rgb'\n",
        "    img_generator = datagen.flow_from_directory(img_path, target_size=IMG_SIZE, color_mode=class_mode_selection,class_mode=None,  batch_size=BATCH_SIZE, seed=SEED)\n",
        "    msk_generator = datagen.flow_from_directory(msk_path, target_size=IMG_SIZE, color_mode=class_mode_selection,class_mode=None, batch_size=BATCH_SIZE, seed=SEED)\n",
        "\n",
        "    return zip(img_generator, msk_generator)\n",
        "\n",
        "# Remember not to perform any image augmentation in the test generator!\n",
        "def create_segmentation_generator_test(img_path, msk_path, BATCH_SIZE):\n",
        "    data_gen_args = dict(rescale=1./255)\n",
        "    datagen = ImageDataGenerator(**data_gen_args)\n",
        "    class_mode_selection = 'grayscale'\n",
        "    if TRANSFER_LEARNING or CLASSIFIER:\n",
        "      class_mode_selection = 'rgb'\n",
        "    img_generator = datagen.flow_from_directory(img_path, target_size=IMG_SIZE, color_mode=class_mode_selection,class_mode=None,  batch_size=BATCH_SIZE, seed=SEED)\n",
        "    msk_generator = datagen.flow_from_directory(msk_path, target_size=IMG_SIZE, color_mode=class_mode_selection,class_mode=None,  batch_size=BATCH_SIZE, seed=SEED)\n",
        "\n",
        "    return zip(img_generator, msk_generator)\n",
        "\n",
        "def create_segmentation_generator_validation(img_path, msk_path, BATCH_SIZE):\n",
        "    data_gen_args = dict(rescale=1./255)\n",
        "    datagen = ImageDataGenerator(**data_gen_args)\n",
        "    class_mode_selection = 'grayscale'\n",
        "    if TRANSFER_LEARNING or CLASSIFIER:\n",
        "      class_mode_selection = 'rgb'\n",
        "    img_generator = datagen.flow_from_directory(img_path, target_size=IMG_SIZE, color_mode=class_mode_selection,class_mode=None,  batch_size=BATCH_SIZE, seed=SEED)\n",
        "    msk_generator = datagen.flow_from_directory(msk_path, target_size=IMG_SIZE, color_mode=class_mode_selection,class_mode=None,  batch_size=BATCH_SIZE, seed=SEED)\n",
        "    return zip(img_generator, msk_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9-MkgwymB3g",
        "outputId": "d14555e9-aa74-45e5-d088-da7fe1572c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9314 images belonging to 1 classes.\n",
            "Found 9314 images belonging to 1 classes.\n",
            "Found 2336 images belonging to 1 classes.\n",
            "Found 2336 images belonging to 1 classes.\n",
            "Found 1052 images belonging to 1 classes.\n",
            "Found 1052 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# load all the data into generetors that will be used in train and predict\n",
        "train_generator = create_segmentation_generator_train(data_dir_train_image, data_dir_train_mask, BATCH_SIZE_TRAIN)\n",
        "test_generator = create_segmentation_generator_test(data_dir_test_image, data_dir_test_mask, BATCH_SIZE_TEST)\n",
        "validation_generator = create_segmentation_generator_validation(data_dir_val_image, data_dir_val_mask, BATCH_SIZE_VALIDATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9clzp0AmEKc"
      },
      "outputs": [],
      "source": [
        "def display(display_list):\n",
        "    plt.figure(figsize=(15,15))\n",
        "\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]), cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj4QuiB5mGGl"
      },
      "outputs": [],
      "source": [
        "def show_dataset(datagen, num=1):\n",
        "    for i in range(0,num):\n",
        "        image,mask = next(datagen)\n",
        "        display([image[0], mask[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMwmjrKEmIAE"
      },
      "outputs": [],
      "source": [
        "# test dataset loaded correctly\n",
        "show_dataset(train_generator, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdfVw5BZD2SM"
      },
      "source": [
        "# Unets\n",
        "Base Unet\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyFR5VJdmLiM"
      },
      "outputs": [],
      "source": [
        "def unet(n_levels, initial_features=32, n_blocks=2, kernel_size=3, pooling_size=2, in_channels=1, out_channels=1):\n",
        "    inputs = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, in_channels))\n",
        "    x = inputs\n",
        "\n",
        "    convpars = dict(kernel_size=kernel_size, activation='relu', padding='same')\n",
        "\n",
        "    #downstream\n",
        "    skips = {}\n",
        "    for level in range(n_levels):\n",
        "        for _ in range(n_blocks):\n",
        "            x = keras.layers.Conv2D(initial_features * (2 ** level), **convpars)(x)\n",
        "        if level < n_levels - 1:\n",
        "            skips[level] = x\n",
        "            x = keras.layers.MaxPool2D(pooling_size)(x)\n",
        "\n",
        "    # upstream\n",
        "    for level in reversed(range(n_levels-1)):\n",
        "        x = keras.layers.Conv2DTranspose(initial_features * (2 ** level), strides=pooling_size, **convpars)(x)\n",
        "        x = keras.layers.Concatenate()([x, skips[level]])\n",
        "        for _ in range(n_blocks):\n",
        "            x = keras.layers.Conv2D(initial_features * 2 ** level, **convpars)(x)\n",
        "\n",
        "    # output\n",
        "    activation = 'sigmoid' if out_channels == 1 else 'softmax'\n",
        "    x = keras.layers.Conv2D(out_channels, kernel_size=1, activation=activation, padding='same')(x)\n",
        "\n",
        "    return keras.Model(inputs=[inputs], outputs=[x], name=f'UNET-L{n_levels}-F{initial_features}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTkQMadVD719"
      },
      "source": [
        "Attention Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwrRle8fjU3l"
      },
      "outputs": [],
      "source": [
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.layers import Conv2D,GlobalAveragePooling2D\n",
        "from keras.applications import ResNet50,VGG16\n",
        "\n",
        "class attention_unet():\n",
        "  def __init__(self,img_rows=512,img_cols=512,tl=False,classifier=False, extra_layer = False):\n",
        "    self.img_rows=img_rows\n",
        "    self.img_cols=img_cols\n",
        "    # if using transfer learning or classifier there is a need for 3 channel input\n",
        "    if tl or classifier:\n",
        "      self.img_shape=(self.img_rows,self.img_cols,3)\n",
        "    else:\n",
        "      self.img_shape=(self.img_rows,self.img_cols,1)\n",
        "    self.df=64\n",
        "    self.uf=64\n",
        "    self.tl=tl\n",
        "    self.classifier=classifier\n",
        "    self.extra_layer = extra_layer\n",
        "\n",
        "  def build_unet(self):\n",
        "    #single encoder block\n",
        "    def conv2d(layer_input,filters,dropout_rate=0,bn=False):\n",
        "      d = conv1d(layer_input,filters,dropout_rate,bn)\n",
        "      d = conv1d(d,filters,dropout_rate,bn)\n",
        "      if dropout_rate:\n",
        "        d=layers.Dropout(dropout_rate)(d)\n",
        "      return d\n",
        "\n",
        "    def conv1d(layer_input,filters,dropout_rate=0,bn=False):\n",
        "      d=layers.Conv2D(filters,kernel_size=(3,3),strides=(1,1),padding='same')(layer_input)\n",
        "      if bn:\n",
        "        d=layers.BatchNormalization()(d)\n",
        "      d=layers.Activation('relu')(d)\n",
        "      return d\n",
        "\n",
        "    # single decoder block\n",
        "    def deconv2d(layer_input,filters,bn=False):\n",
        "      u=layers.UpSampling2D((2,2))(layer_input)\n",
        "      u=layers.Conv2D(filters,kernel_size=(3,3),strides=(1,1),padding='same')(u)\n",
        "      if bn:\n",
        "        u=layers.BatchNormalization()(u)\n",
        "      u=layers.Activation('relu')(u)\n",
        "      return u\n",
        "\n",
        "    # attention gate mechanism for skip connections\n",
        "    def attention_block(F_g,F_l,F_int,bn=False):\n",
        "      g=layers.Conv2D(F_int,kernel_size=(1,1),strides=(1,1),padding='valid')(F_g)\n",
        "      if bn:\n",
        "        g=layers.BatchNormalization()(g)\n",
        "      x=layers.Conv2D(F_int,kernel_size=(1,1),strides=(1,1),padding='valid')(F_l)\n",
        "      if bn:\n",
        "        x=layers.BatchNormalization()(x)\n",
        "      psi=layers.Add()([g,x])\n",
        "      psi=layers.Activation('relu')(psi)\n",
        "      psi=layers.Conv2D(1,kernel_size=(1,1),strides=(1,1),padding='valid')(psi)\n",
        "      if bn:\n",
        "        psi=layers.BatchNormalization()(psi)\n",
        "      psi=layers.Activation('sigmoid')(psi)\n",
        "\n",
        "      return layers.Multiply()([F_l,psi])\n",
        "\n",
        "    # init model\n",
        "    inputs = layers.Input(shape=self.img_shape, name=\"input_1\")\n",
        "    if self.tl:  # if tranfer learning get encoder blcoks from resnet50 model\n",
        "      resnet50 = ResNet50(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
        "      conv1 = resnet50.get_layer(\"input_1\").output\n",
        "      conv2 = resnet50.get_layer(\"conv1_relu\").output\n",
        "      conv3 = resnet50.get_layer(\"conv2_block3_out\").output\n",
        "      conv4 = resnet50.get_layer(\"conv3_block4_out\").output\n",
        "      if self.extra_layer:  # if using extra layer get an extra layer from resnet50\n",
        "        conv5 = resnet50.get_layer(\"conv4_block6_out\").output\n",
        "        pool5=layers.MaxPooling2D((2,2))(conv5)\n",
        "        mid_block=conv2d(pool5,self.df*16,dropout_rate=0.5,bn=True)\n",
        "      else:\n",
        "        mid_block=resnet50.get_layer(\"conv4_block6_out\").output\n",
        "    else:\n",
        "      conv1=conv2d(inputs,self.df)\n",
        "      pool1=layers.MaxPooling2D((2,2))(conv1)\n",
        "\n",
        "      conv2=conv2d(pool1,self.df*2,bn=True)\n",
        "      pool2=layers.MaxPooling2D((2,2))(conv2)\n",
        "      conv3=conv2d(pool2,self.df*4,bn=True)\n",
        "      pool3=layers.MaxPooling2D((2,2))(conv3)\n",
        "\n",
        "      conv4=conv2d(pool3,self.df*8,bn=True)\n",
        "      pool4=layers.MaxPooling2D((2,2))(conv4)\n",
        "\n",
        "      if self.extra_layer:\n",
        "        conv5 = conv2d(pool4,self.df*8,dropout_rate=0.5,bn=True)\n",
        "        pool5=layers.MaxPooling2D((2,2))(conv5)\n",
        "        mid_block=conv2d(pool5,self.df*16,dropout_rate=0.5,bn=True)\n",
        "      else:\n",
        "        mid_block = mid_block=conv2d(pool4,self.df*16,dropout_rate=0.5,bn=True)\n",
        "\n",
        "    if self.extra_layer:  # if we have an extra encoder layer we will need extra decoder layer\n",
        "      up5 = deconv2d(mid_block,self.uf*8,bn=True)\n",
        "      extra_conv=attention_block(up5,conv5,self.uf*8,bn=True)\n",
        "      extra_up=layers.Concatenate()([up5,extra_conv])\n",
        "      extra_conv=conv2d(extra_up,self.uf*8)\n",
        "      mid_block = extra_conv\n",
        "\n",
        "    up6=deconv2d(mid_block,self.uf*8,bn=True)\n",
        "    conv6=attention_block(up6,conv4,self.uf*8,bn=True)\n",
        "    up6=layers.Concatenate()([up6,conv6])\n",
        "    conv6=conv2d(up6,self.uf*8)\n",
        "\n",
        "    up7=deconv2d(conv6,self.uf*4,bn=True)\n",
        "    conv7=attention_block(up7,conv3,self.uf*4,bn=True)\n",
        "    up7=layers.Concatenate()([up7,conv7])\n",
        "    conv7=conv2d(up7,self.uf*4)\n",
        "\n",
        "    up8=deconv2d(conv7,self.uf*2,bn=True)\n",
        "    conv8=attention_block(up8,conv2,self.uf*2,bn=True)\n",
        "    up8=layers.Concatenate()([up8,conv8])\n",
        "    conv8=conv2d(up8,self.uf*2)\n",
        "\n",
        "    up9=deconv2d(conv8,self.uf,bn=True)\n",
        "    conv9=attention_block(up9,conv1,self.uf,bn=True)\n",
        "    up9=layers.Concatenate()([up9,conv9])\n",
        "    conv9=conv2d(up9,self.uf)\n",
        "\n",
        "    if self.classifier or self.tl:\n",
        "      output_channel = 3\n",
        "    else:\n",
        "      output_channel = 1\n",
        "    if self.classifier:  # build classifier block using mid-block feature map concatinated with vgg16\n",
        "      outputs=layers.Conv2D(output_channel,kernel_size=(1,1),strides=(1,1),activation='sigmoid')(conv9)\n",
        "      vgg16 = VGG16(include_top=False, weights=\"imagenet\", input_shape=self.img_shape,input_tensor=inputs)\n",
        "      for layer in vgg16.layers:\n",
        "            layer.trainable = False\n",
        "      b1 = vgg16.get_layer(\"block5_conv3\").output\n",
        "      concateBridge=layers.Concatenate()([mid_block,b1])\n",
        "      cls = Conv2D(32, (3,3), activation='relu', padding='same')(concateBridge)  # connected to mid_block\n",
        "      cls = Conv2D(3, (1,1))(cls)\n",
        "      cls = GlobalAveragePooling2D()(cls)\n",
        "      cls = layers.Activation('sigmoid')(cls)\n",
        "      clsr = layers.Reshape((1, 1, 3), name='classification')(cls)\n",
        "      out_with_cls = layers.multiply(inputs=[outputs,clsr], name='segmentation') # if using classifier the output of the model will be multiplied between segmentation result and classification result\n",
        "      model=Model(inputs=inputs,outputs=out_with_cls,name=\"Attention-UNET\")\n",
        "    else:\n",
        "      outputs=layers.Conv2D(output_channel,kernel_size=(1,1),strides=(1,1),activation='sigmoid')(conv9)\n",
        "      model=Model(inputs=inputs,outputs=outputs,name=\"Attention-UNET\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nlh963ND-4-"
      },
      "source": [
        "Unet 3 plus (no deep no cgm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDXjGqHPptzu"
      },
      "outputs": [],
      "source": [
        "#https://github.com/hamidriasat/UNet-3-Plus/blob/69bcb36169191f57846029e6d2592c750ae8b7d6/models/unet3plus.py\n",
        "import keras as k\n",
        "def conv_block(x, kernels, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
        "               is_bn=True, is_relu=True, n=2):\n",
        "    \"\"\" Custom function for conv2d:\n",
        "        Apply  3*3 convolutions with BN and relu.\n",
        "    \"\"\"\n",
        "    for i in range(1, n + 1):\n",
        "        x = k.layers.Conv2D(filters=kernels, kernel_size=kernel_size,\n",
        "                            padding=padding, strides=strides,\n",
        "                            kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "                            kernel_initializer=k.initializers.he_normal(seed=5))(x)\n",
        "        if is_bn:\n",
        "            x = k.layers.BatchNormalization()(x)\n",
        "        if is_relu:\n",
        "            x = k.activations.relu(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def unet3plus(input_shape, output_channels):\n",
        "    \"\"\" UNet3+ base model \"\"\"\n",
        "    filters = [64, 128, 256, 512, 1024]\n",
        "\n",
        "    input_layer = k.layers.Input(\n",
        "        shape=input_shape,\n",
        "        name=\"input_layer\"\n",
        "    )  # 320*320*3\n",
        "\n",
        "    \"\"\" Encoder\"\"\"\n",
        "    # block 1\n",
        "    e1 = conv_block(input_layer, filters[0])  # 320*320*64\n",
        "\n",
        "    # block 2\n",
        "    e2 = k.layers.MaxPool2D(pool_size=(2, 2))(e1)  # 160*160*64\n",
        "    e2 = conv_block(e2, filters[1])  # 160*160*128\n",
        "\n",
        "    # block 3\n",
        "    e3 = k.layers.MaxPool2D(pool_size=(2, 2))(e2)  # 80*80*128\n",
        "    e3 = conv_block(e3, filters[2])  # 80*80*256\n",
        "\n",
        "    # block 4\n",
        "    e4 = k.layers.MaxPool2D(pool_size=(2, 2))(e3)  # 40*40*256\n",
        "    e4 = conv_block(e4, filters[3])  # 40*40*512\n",
        "\n",
        "    # block 5\n",
        "    # bottleneck layer\n",
        "    e5 = k.layers.MaxPool2D(pool_size=(2, 2))(e4)  # 20*20*512\n",
        "    e5 = conv_block(e5, filters[4])  # 20*20*1024\n",
        "\n",
        "    \"\"\" Decoder \"\"\"\n",
        "    cat_channels = filters[0]\n",
        "    cat_blocks = len(filters)\n",
        "    upsample_channels = cat_blocks * cat_channels\n",
        "\n",
        "    \"\"\" d4 \"\"\"\n",
        "    e1_d4 = k.layers.MaxPool2D(pool_size=(8, 8))(e1)  # 320*320*64  --> 40*40*64\n",
        "    e1_d4 = conv_block(e1_d4, cat_channels, n=1)  # 320*320*64  --> 40*40*64\n",
        "\n",
        "    e2_d4 = k.layers.MaxPool2D(pool_size=(4, 4))(e2)  # 160*160*128 --> 40*40*128\n",
        "    e2_d4 = conv_block(e2_d4, cat_channels, n=1)  # 160*160*128 --> 40*40*64\n",
        "\n",
        "    e3_d4 = k.layers.MaxPool2D(pool_size=(2, 2))(e3)  # 80*80*256  --> 40*40*256\n",
        "    e3_d4 = conv_block(e3_d4, cat_channels, n=1)  # 80*80*256  --> 40*40*64\n",
        "\n",
        "    e4_d4 = conv_block(e4, cat_channels, n=1)  # 40*40*512  --> 40*40*64\n",
        "\n",
        "    e5_d4 = k.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(e5)  # 80*80*256  --> 40*40*256\n",
        "    e5_d4 = conv_block(e5_d4, cat_channels, n=1)  # 20*20*1024  --> 20*20*64\n",
        "\n",
        "    d4 = k.layers.concatenate([e1_d4, e2_d4, e3_d4, e4_d4, e5_d4])\n",
        "    d4 = conv_block(d4, upsample_channels, n=1)  # 40*40*320  --> 40*40*320\n",
        "\n",
        "    \"\"\" d3 \"\"\"\n",
        "    e1_d3 = k.layers.MaxPool2D(pool_size=(4, 4))(e1)  # 320*320*64 --> 80*80*64\n",
        "    e1_d3 = conv_block(e1_d3, cat_channels, n=1)  # 80*80*64 --> 80*80*64\n",
        "\n",
        "    e2_d3 = k.layers.MaxPool2D(pool_size=(2, 2))(e2)  # 160*160*256 --> 80*80*256\n",
        "    e2_d3 = conv_block(e2_d3, cat_channels, n=1)  # 80*80*256 --> 80*80*64\n",
        "\n",
        "    e3_d3 = conv_block(e3, cat_channels, n=1)  # 80*80*512 --> 80*80*64\n",
        "\n",
        "    e4_d3 = k.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(d4)  # 40*40*320 --> 80*80*320\n",
        "    e4_d3 = conv_block(e4_d3, cat_channels, n=1)  # 80*80*320 --> 80*80*64\n",
        "\n",
        "    e5_d3 = k.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(e5)  # 20*20*320 --> 80*80*320\n",
        "    e5_d3 = conv_block(e5_d3, cat_channels, n=1)  # 80*80*320 --> 80*80*64\n",
        "\n",
        "    d3 = k.layers.concatenate([e1_d3, e2_d3, e3_d3, e4_d3, e5_d3])\n",
        "    d3 = conv_block(d3, upsample_channels, n=1)  # 80*80*320 --> 80*80*320\n",
        "\n",
        "    \"\"\" d2 \"\"\"\n",
        "    e1_d2 = k.layers.MaxPool2D(pool_size=(2, 2))(e1)  # 320*320*64 --> 160*160*64\n",
        "    e1_d2 = conv_block(e1_d2, cat_channels, n=1)  # 160*160*64 --> 160*160*64\n",
        "\n",
        "    e2_d2 = conv_block(e2, cat_channels, n=1)  # 160*160*256 --> 160*160*64\n",
        "\n",
        "    d3_d2 = k.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(d3)  # 80*80*320 --> 160*160*320\n",
        "    d3_d2 = conv_block(d3_d2, cat_channels, n=1)  # 160*160*320 --> 160*160*64\n",
        "\n",
        "    d4_d2 = k.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(d4)  # 40*40*320 --> 160*160*320\n",
        "    d4_d2 = conv_block(d4_d2, cat_channels, n=1)  # 160*160*320 --> 160*160*64\n",
        "\n",
        "    e5_d2 = k.layers.UpSampling2D(size=(8, 8), interpolation='bilinear')(e5)  # 20*20*320 --> 160*160*320\n",
        "    e5_d2 = conv_block(e5_d2, cat_channels, n=1)  # 160*160*320 --> 160*160*64\n",
        "\n",
        "    d2 = k.layers.concatenate([e1_d2, e2_d2, d3_d2, d4_d2, e5_d2])\n",
        "    d2 = conv_block(d2, upsample_channels, n=1)  # 160*160*320 --> 160*160*320\n",
        "\n",
        "    \"\"\" d1 \"\"\"\n",
        "    e1_d1 = conv_block(e1, cat_channels, n=1)  # 320*320*64 --> 320*320*64\n",
        "\n",
        "    d2_d1 = k.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(d2)  # 160*160*320 --> 320*320*320\n",
        "    d2_d1 = conv_block(d2_d1, cat_channels, n=1)  # 160*160*320 --> 160*160*64\n",
        "\n",
        "    d3_d1 = k.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(d3)  # 80*80*320 --> 320*320*320\n",
        "    d3_d1 = conv_block(d3_d1, cat_channels, n=1)  # 320*320*320 --> 320*320*64\n",
        "\n",
        "    d4_d1 = k.layers.UpSampling2D(size=(8, 8), interpolation='bilinear')(d4)  # 40*40*320 --> 320*320*320\n",
        "    d4_d1 = conv_block(d4_d1, cat_channels, n=1)  # 320*320*320 --> 320*320*64\n",
        "\n",
        "    e5_d1 = k.layers.UpSampling2D(size=(16, 16), interpolation='bilinear')(e5)  # 20*20*320 --> 320*320*320\n",
        "    e5_d1 = conv_block(e5_d1, cat_channels, n=1)  # 320*320*320 --> 320*320*64\n",
        "\n",
        "    d1 = k.layers.concatenate([e1_d1, d2_d1, d3_d1, d4_d1, e5_d1, ])\n",
        "    d1 = conv_block(d1, upsample_channels, n=1)  # 320*320*320 --> 320*320*320\n",
        "\n",
        "    # last layer does not have batchnorm and relu\n",
        "    d = conv_block(d1, output_channels, n=1, is_bn=False, is_relu=False)\n",
        "\n",
        "    output = k.activations.softmax(d)\n",
        "\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=[output], name='UNet_3Plus')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "levwwHhMEGm8"
      },
      "source": [
        "# loss functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O88rhgjZh8cG"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "def dice_coefficient(y_true, y_pred, smooth=1):\n",
        "    # Flatten the inputs\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "\n",
        "    # Calculate the intersection and union\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f)\n",
        "\n",
        "    # Calculate the Dice coefficient\n",
        "    dice_coef = (2.0 * intersection + smooth) / (union + smooth)\n",
        "\n",
        "    return dice_coef\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1 - dice_coefficient(y_true, y_pred)\n",
        "\n",
        "def tversky(y_true, y_pred,smooth = 1):\n",
        "    y_true_pos = K.flatten(y_true)\n",
        "    y_pred_pos = K.flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
        "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
        "    alpha = 0.7\n",
        "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
        "\n",
        "def tversky_loss(y_true, y_pred):\n",
        "    return 1 - tversky(y_true,y_pred)\n",
        "\n",
        "def focal_tversky(y_true,y_pred):\n",
        "    pt_1 = tversky(y_true, y_pred)\n",
        "    gamma = 0.75\n",
        "    return K.pow((1-pt_1), gamma)\n",
        "\n",
        "# Evaluation metrics: iou\n",
        "def iou(y_true, y_pred, smooth = 1.):\n",
        "    intersection = K.sum(y_true * y_pred)\n",
        "    sum_ = K.sum(y_true) + K.sum(y_pred)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return jac\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vniWhVcMEmj-"
      },
      "source": [
        "# Compile, train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llqEigJUzh00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58329e63-a494-4dbb-f34a-8eb10641ed7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n"
          ]
        }
      ],
      "source": [
        "if UNET_MODEL_TO_TRAIN ==  \"unet\":\n",
        "    model = unet(4)\n",
        "if UNET_MODEL_TO_TRAIN == \"attention\":\n",
        "    a=attention_unet(img_rows=IMAGE_HEIGHT, img_cols=IMAGE_WIDTH,classifier=CLASSIFIER, tl = TRANSFER_LEARNING , extra_layer = EXTRA_LAYER)\n",
        "    model=a.build_unet()\n",
        "if UNET_MODEL_TO_TRAIN == \"3plus\":\n",
        "    INPUT_SHAPE = [IMAGE_HEIGHT, IMAGE_WIDTH, 1]\n",
        "    OUTPUT_CHANNELS = 1\n",
        "    model = unet3plus(INPUT_SHAPE, OUTPUT_CHANNELS)\n",
        "if LOSS_FUNC_DICE:\n",
        "  loss_func=dice_coef_loss\n",
        "  metric_func=dice_coefficient\n",
        "if LOSS_FUNC_TRAV:\n",
        "  loss_func=focal_tversky\n",
        "  metric_func=tversky\n",
        "if USE_IOU:\n",
        "  metric_func=iou\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss=loss_func, metrics=metric_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWm7_BQymPfB"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQsWAMVEmQGE"
      },
      "outputs": [],
      "source": [
        "#  train the selcted model\n",
        "history=model.fit(train_generator,\n",
        "                    steps_per_epoch=EPOCH_STEP_TRAIN,\n",
        "                    validation_data=test_generator,\n",
        "                    validation_steps=EPOCH_STEP_TEST,\n",
        "                   epochs=NUM_OF_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZnJIJTtMFTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d290f96d-f7fe-4bd5-8fa1-2f22c34b1b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to disk.\n"
          ]
        }
      ],
      "source": [
        "# use this section for saving the model weights and history for later use\n",
        "\n",
        "loss_name=\"\"\n",
        "if LOSS_FUNC_DICE:\n",
        "  loss_name=\"Dice\"\n",
        "if LOSS_FUNC_TRAV:\n",
        "  loss_name=\"Tver\"\n",
        "# Serialize model to JSON:\n",
        "model_json = model.to_json()\n",
        "with open(f'/content/drive/My Drive/KneeProject/models/{model.name}_({IMAGE_HEIGHT}_{IMAGE_WIDTH})_epochs:{NUM_OF_EPOCHS}_learning:{LEARNING_RATE}_loss:{loss_name}_batch:{BATCH_SIZE_TRAIN}.json', \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Serialize weights to HDF5 (h5py needed):\n",
        "model.save_weights(f'/content/drive/My Drive/KneeProject/models/{model.name}_({IMAGE_HEIGHT}_{IMAGE_WIDTH})_epochs:{NUM_OF_EPOCHS}_learning:{LEARNING_RATE}_loss:{loss_name}_batch:{BATCH_SIZE_TRAIN}.h5')\n",
        "\n",
        "with open(f'/content/drive/My Drive/KneeProject/models/{model.name}_({IMAGE_HEIGHT}_{IMAGE_WIDTH})_epochs:{NUM_OF_EPOCHS}_learning:{LEARNING_RATE}_loss:{loss_name}_batch:{BATCH_SIZE_TRAIN}.pkl', 'wb') as f:\n",
        "    pickle.dump(history, f)\n",
        "\n",
        "print(\"Model saved to disk.\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UdfVw5BZD2SM",
        "levwwHhMEGm8",
        "vniWhVcMEmj-"
      ],
      "provenance": [],
      "gpuType": "A100",
      "gpuClass": "premium",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}